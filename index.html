<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>Kaiying</title>
  <meta property="og:title" content="Kaiying" />
  <meta name="twitter:title" content="Kaiying" />
  <meta name="author" content="kaiying"/>
  <meta property="og:site_name" content="Kaiying" />
  <meta property="og:url" content="https://wukaiying.github.io/" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.61.0" />
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Kaiying" />
  <link href="/index.xml" rel="feed" type="application/rss+xml" title="Kaiying" />

  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="/js/script.js"></script>
  <script src="/js/custom.js"></script>
  <script defer src="/js/fontawesome.js"></script>
</head>

<body>

<header class="site-header">
  <nav class="site-navi">
    <h1 class="site-title"><a href="/">Kaiying</a></h1>
    <ul class="site-navi-items">
      <li class="site-navi-item-categories"><a href="/categories/" title="Categories">Categories</a></li>
      <li class="site-navi-item-tags"><a href="/tags/" title="Tags">Tags</a></li>
      <li class="site-navi-item-archives"><a href="/archives/" title="Archives">Archives</a></li>
      <li class="site-navi-item-about"><a href="/about/" title="About">About</a></li>
    </ul>
  </nav>
</header>
<hr class="site-header-bottom">

  <div class="main" role="main">
    <section class="list home-list">
      <article class="article">
        <a href="/golang/go-plugin/" class="article-titles">
          <h2 class="article-title">golang 实现加载动态库</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 27, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/golang/">
              <i class="fas fa-folder"></i>
              golang
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/go/">
              <i class="fas fa-tag"></i>
              go
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/plugin/">
              <i class="fas fa-tag"></i>
              plugin
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          golang开发中需要用到插件动态加载技术，需要主程序动态加载一些功能模块，不需要修改主程序。相比于java ,java 可以通过加载class文件方式来实现模块加载，而golang通常都是单个二进制文件。 go-plugin则使用另外一种方式实现插件加载，主进程和插件进程是两个独立的进程，二者通过rpc/grpc的方式进行通信， 从而实现主进程调用插件进程。
其次，golang也原生提供一种插件模块加载机制plugins，下面依次进行介绍。
go-plugin 项目地址https://github.com/hashicorp/go-plugin
特征  插件是Go接口的实现：这让插件的编写、使用非常自然。对于插件的作者来说，他只需要实现一个Go接口即可；对于插件的用户来说，他只需要调用一个Go接口即可。go-plugin会处理好本地调用转换为gRPC调用的所有细节 跨语言支持：插件可以基于任何主流语言编写，同样可以被任何主流语言消费 支持复杂的参数、返回值：go-plugin可以处理接口、io.Reader/Writer等复杂类型 双向通信：为了支持复杂参数，宿主进程能够将接口实现发送给插件，插件也能够回调到宿主进程 内置日志系统：任何使用log标准库的的插件，都会将日志信息传回宿主机进程。宿主进程会在这些日志前面加上插件二进制文件的路径，并且打印日志 协议版本化：支持一个简单的协议版本化，增加版本号后可以基于老版本协议的插件无效化。当接口签名变化时应当增加版本 标准输出/错误同步：插件以子进程的方式运行，这些子进程可以自由的使用标准输出/错误，并且打印的内容会被自动同步到宿主进程，宿主进程可以为同步的日志指定一个io.Writer TTY Preservation：插件子进程可以链接到宿主进程的stdin文件描述符，以便要求TTY的软件能正常工作 宿主进程升级：宿主进程升级的时候，插件子进程可以继续允许，并在升级后自动关联到新的宿主进程 加密通信：gRPC信道可以加密 完整性校验：支持对插件的二进制文件进行Checksum 插件崩溃了，不会导致宿主进程崩溃 容易安装：只需要将插件放到某个宿主进程能够访问的目录即可  用法 这里我们通过一个实例来体验一下go-plugin插件开发。我们需要完成的功能是，主程序暴露了两个方法，插件模块去实现这两个方法，然后主程序加载插件完成功能实现。 最终达成的效果是通过Put方法设置一个key/value对，通过get方法获取key对应的值。
Put(key string, value []byte) error Get(key string) ([]byte, error) ([]byte, error) 新建项目 这里我先将完整项目目录结构列出来
. ├── Makefile ├── cmd │ ├── cmd │ ├── kv_wky │ └── main.go ├── pkg │ ├── plugins │ │ └── proto │ │ ├── plugin.pb.go │ │ └── plugin.proto │ └── shared │ ├── grpc.go │ └── plugin.go └── pluginclient ├── main.go └── pluginclient 定义proto 新建pkg/plugins/proto/plugin.proto文件，并生成plugin.pb.go
syntax = &#34;proto3&#34;;package proto;message GetRequest { string key = 1;}message GetResponse { bytes value = 1;}message PutRequest { string key = 1; bytes value = 2;}message Empty {}service KV { rpc Get(GetRequest) returns (GetResponse); rpc Put(PutRequest) returns (Empty);} 定义对外暴露的插件接口 新建pkg/shared/plugin.
        </div>
        <div class="article-readmore"><a href="/golang/go-plugin/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/calico-source-code/" class="article-titles">
          <h2 class="article-title">Calico 学习及源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>May 7, 2020</time></li>
        </ul>
        <div class="article-content">
          背景 如何选取合适的网络插件：
1.集群环境因素
如果集群节点机器为虚拟机，不允许机器之间直接二层互联（所谓二层互联也就是说通过交换机可以直接通信，而可以通过交换机直接通信则说明通过交换机互联的机器处于同一个子网中，不同子网直接通信则需要用到路由器）， 必须要带有IP地址这种三层的内容才能去做转发，或者限制机器只能使用某些IP等约束，这种情况只能使用overlay模式去实现。常见的有Flannel-vxlan,Calico-ipip,Weave等。
物理机环境同一个子网底层网络可以通过交换机直接通信，此时我们可以使用underlay模式或者路由模式的网络插件。这样避免了使用vxlan封包解包d导致性能降低，常见的插件有Flannel-hostgw,Calico-bgp等。
2.性能因素
pod 创建速度，在pod需要扩容时，overlay模式创建pod速度快，内核接口就可以完成这些操作。underlay模式需要准备底层网络资源，速度较慢。
其次，overlay模式包含封包解包，会带来一些包头的损失、CPU 的消耗等，对网络性能的要求比较高，比如说机器学习、大数据这些场景就不适合使用 Overlay 模式。
calico calico网络模型主要的组件
 Felix:  每一个节点上的agent进程，监听etcd，当有新节点加入或者新的容器创建，felix为它设置网卡，IP,MAC，然后更新节点路由表，添加策略到Iptables
  etcd: 数据存储
  BGP Client
  每一个节点上都有BGP Client，监听felix写入的路由信息，然后通过BGP协议广播到其他Host节点上去，实现网络互通。该模式比较适合集群规模较小的场景，因为每一个BGP Client需要和集群其他节点互连。集群规模较大时，路由表会被撑满。
 BGP Route Reflector:  针对BGP Client存在的问题，对于大规模集群，可以采用BGP的Router Reflector的方法，使所有BGP Client仅与特定Router Reflector节点互联并做路由同步，从而大大减少连接数。 calico 网络模式 calico 作为容器网络方案最大的特点就是它提供了纯三层网络报文转发方案，也就是说每个节点中的容器都可以通过IP地址来直接通信，中间可以通过路由转发找到对方。对比于flannel-hostgw方案，虽然也是通过路由实现通信，但是它依赖于集群所有节点二层连通，然后通过更新每个节点路由的方式实现通信，也就是说flannel-hostgw 方案需要所有节点在同一个子网中，不在同一个子网无法通信。
calico可以实现跨子网通信，是因为他在每一个集群节点中加入了虚拟路由(vrouter)，通过BGP路由协议，使得每一个节点中的路由信息可以被传播到其他路由设备，实现三层网络通信。                                                                                                                                                                                                         但是，并不是所有的网络都支持BGP路由协议，calico也提供了overlay模式解决方案&mdash;ipip模式
IPIP模式 和其他overlay模式实现的原理一样，ipip模式是在各个节点之间架起一个隧道，使用节点IP封装原始IP报文，启用ipip模式是，calico会在每一个节点上创建一个名字为tunl0的虚拟网络接口。
ping数据包流向图
我们来分析一下一个集群中处于两个不同node的pod数据包是如何传输的，集群网络模式是calico-ipip
node2节点ip为172.31.127.252，node2中pod2的ip为10.100.9.206
node1节点ip为172.31.112.2，node1中pod1的ip为10.100.15.150，如下图所示：
 pod网络分析  首先进入pod1
$ kubectl exec -it pod1 /bin/bash 查看pod1中包含的网络设备，可以看到pod1中只包含lo和eth0
$ ip addr 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if26: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1440 qdisc noqueue state UP group default link/ether ce:83:2b:89:af:9e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.
        </div>
        <div class="article-readmore"><a href="/k8s/calico-source-code/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kube-proxy-source-code-read/" class="article-titles">
          <h2 class="article-title">K8S Proxy 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 29, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          前言 在分析kube-proxy之前，我们需要先知道k8s中另外一个概念service。
service 是一组具有相同 label pod 集合的抽象，集群内外的各个服务可以通过 service 进行互相访问。
当创建一个 service 对象时也会对应创建一个 endpoint 对象，endpoint 存放的是pod服务的ip地址，service 只是将多个pod进行关联起来。
当用户通过service访问后端pod服务时，实际的路由转发都是由 kubernetes 中的 kube-proxy 组件来实现，因此，service 必须结合 kube-proxy 使用。
kube-proxy 组件可以运行在 kubernetes 集群中的每一个节点上也可以只运行在单独的几个节点上，其会根据 service 和 endpoints 的变动来刷新节点上 iptables 或者 ipvs 中保存的路由规则。
service 工作原理 endpoint controller 负责监听 service 和对应 pod 的变化，更新对应 service 的 endpoints 对象。当用户创建 service 后 endpoints controller 会监听 pod 的状态，当 pod 处于 running 且准备就绪时，endpoints controller 会将 pod ip 记录到 endpoints 对象中。 kube-proxy 会监听 service 和 endpoints 的变化并调用其代理模块在主机上刷新路由规则。
总结就是，endpoint controller负责更新service ip和其对应后端服务endpoint ip的关系。而kube-proxy则负责更新各个节点的iptables或者ipvs使得流量能够被正常转发。
service 暴露服务方式 ClusterIp ClusterIP 类型的 service 是 kubernetes 集群默认的服务暴露方式，它只能用于集群内部通信，可以被各 pod 访问，其访问方式为：
pod ---&gt; ClusterIP:ServicePort --&gt; (iptables)DNAT --&gt; PodIP:containePort NodePort client ---&gt; NodeIP:NodePort ---&gt; ClusterIP:ServicePort ---&gt; (iptables)DNAT ---&gt; PodIP:containePort LoadBalancer Ingress kube-proxy 模式 kube-proxy 的路由转发规则是通过其后端的代理模块实现的，kube-proxy 的代理模块目前有四种实现方案，userspace、iptables、ipvs、kernelspace，其发展历程如下所示：
        </div>
        <div class="article-readmore"><a href="/k8s/kube-proxy-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kubelet-create-pod-lifecycle/" class="article-titles">
          <h2 class="article-title">Kubelet 创建pod流程分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 28, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          前言 上一节我们分析了kubelet启动的基本流程，这一节我们分析一下当有新的pod分配到当前节点时，kubelet如何创建一个pod
syncLoop 上一节最后我们分析到kl.syncLoop(updates, kl)，是kubelet里面一个主循环逻辑，看一下它的具体实现： 它从不同的管道（文件、URL 和 apiserver）监听变化，并把它们汇聚起来。 当有新的变化发生时，它会调用对应的处理函数，保证 pod 处于期望的状态。 如果 pod 没有变化，它也会定期保证所有的容器和最新的期望状态保持一致。这个方法是 for 循环，不会退出。
func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.Info(&#34;Starting kubelet main sync loop.&#34;) syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.Infof(&#34;skipping pod synchronization - %v&#34;, err) // exponential backoff 	time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success 	duration = base kl.
        </div>
        <div class="article-readmore"><a href="/k8s/kubelet-create-pod-lifecycle/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kube-scheduler-priority-source-read/" class="article-titles">
          <h2 class="article-title">Kube Scheduler 优选策略源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 24, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 预选阶段完成后，进入优选阶段，将需要调度的Pod列表和Node列表传入各种优选算法进行打分，最终整合成结果集HostPriorityList
//pkg/scheduler/core/generic_scheduler.go:189 // Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) { ... trace.Step(&#34;Basic checks done&#34;) startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod) if err != nil { return result, err } // Run &#34;postfilter&#34; plugins. 	postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses) if !
        </div>
        <div class="article-readmore"><a href="/k8s/kube-scheduler-priority-source-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kubelet-structure/" class="article-titles">
          <h2 class="article-title">Kubelet 架构分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 23, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kubelet是k8s集群中节点的代理人，通过watch api server，来处理发送给本节点的任务，看一下官方对kubelet的描述：
The kubelet is the primary &quot;node agent&quot; that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy.
        </div>
        <div class="article-readmore"><a href="/k8s/kubelet-structure/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kube-scheduler-source-code-read/" class="article-titles">
          <h2 class="article-title">K8S Scheduler 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 22, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kube-scheduler 是master节点中重要的组件之一，它同watch接口监听api server中需要调度的pod，然后通过其预选，优选算法来确定pod需要被调度到哪个节点上去。
看一下官方对scheduler 调度流程的描述：The Kubernetes Scheduler
主要步骤如下：
 首先通过预选阶段过滤掉不合适的节点，例如根据podSpec中对系统资源的需求，会计算出所有节点剩余的资源是否可以满足pod需求，如果不满足则该节点会被抛弃。 其次通过预选阶段的节点，会再次进行优选。比如在满足pod资源需求的基础上，会优选出当前负载最小节点，保证每个集群节点资源被均衡使用。 最后选择打分最高的节点作为目标节点，如果得分相同则会随机选择一个，进行调度。  For given pod: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Pred. filters: node 3 doesn't have enough resource +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | remaining nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Priority function: node 1: p=2 node 2: p=5 +-------------------+-------------------------+ | | v select max{node priority} = node 2 入口 scheduler使用cobra创建命令行客户端，进入NewSchedulerCommand
        </div>
        <div class="article-readmore"><a href="/k8s/kube-scheduler-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/replicaset-source-code-read/" class="article-titles">
          <h2 class="article-title">Replica Set Controller 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 21, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境 kubernetes版本：kubernetes:v1.16.0
go环境：go version go1.13.4 darwin/amd64
前言 前面文章已经总结到，deployment controller通过replicaset来控制pod的创建和删除，进而实现更高级操作例如，滚动更新，回滚等。本文继续从源码角度 分析replicaset controller如何控制pod的创建和删除。
在分析源码前先考虑一下 replicaset 的使用场景，在平时的操作中其实我们并不会直接操作 replicaset，replicaset 也仅有几个简单的操作，创建、删除、更新等。
源码分析 入口 我们在上一篇文章中也说道controller manager是所有k8s 资源controller的管控中心，包含的主要逻辑就是高可用选择leader，然后 遍历所有的controllers,对每一个controller进行初始化并启动，也就是说只要集群启动，所有的controller也会都启动起来，开始list-watch api server，来准备创建资源。
cmd/kube-controller-manager/app/controllermanager.go:386 func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[&#34;endpoint&#34;] = startEndpointController controllers[&#34;endpointslice&#34;] = startEndpointSliceController ... controllers[&#34;deployment&#34;] = startDeploymentController controllers[&#34;replicaset&#34;] = startReplicaSetController } 接下来进入startReplicaSetController, 发现通过NewReplicaSetController初始化了ReplicaSetController，并并调用run方法启动controller。
cmd/kube-controller-manager/app/apps.go:69 func startReplicaSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: &#34;apps&#34;, Version: &#34;v1&#34;, Resource: &#34;replicasets&#34;}] { return nil, false, nil } go replicaset.NewReplicaSetController( ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(&#34;replicaset-controller&#34;), replicaset.BurstReplicas, ).Run(int(ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs), ctx.Stop) return nil, true, nil } 我们先分析NewReplicaSetController看一下初始化ReplicaSetController的具体步骤：发现他会监听rs的add,update,delete操作，和pod的add,update,delete操作。 当这两种资源发送变化时，将key(name+namespace)添加到queue队列中去。
//pkg/controller/replicaset/replica_set.go:126  func NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.
        </div>
        <div class="article-readmore"><a href="/k8s/replicaset-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/controller-manager-source-code-read/" class="article-titles">
          <h2 class="article-title">K8S Controller Manager 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 19, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 Controller Manager内部包含Replication Controller、Node Controller、 ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Service Controller及Endpoint Controller这8种Controller， 每个Controller，它们通过API Server提供的（List-Watch）接口实时监控集群中特定资源的状态变化，当发生各种故障 导致某资源对象的状态发生变化时，Controller会尝试将其状态调整为期望的状态。
代码总体解析 入口 # cmd/kube-controller-manager/controller-manager.go func main() { rand.Seed(time.Now().UnixNano()) command := app.NewControllerManagerCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 进入NewControllerManagerCommand 函数，可以看到新建ControllerManager需要两步 启动参数配置，及Run函数，进入Run函数。
func NewControllerManagerCommand() *cobra.Command { s, err := options.NewKubeControllerManagerOptions() if err != nil { klog.Fatalf(&#34;unable to initialize command options: %v&#34;, err) } cmd := &amp;cobra.Command{ Use: &#34;kube-controller-manager&#34;, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, &#34;%v\n&#34;, err) os.
        </div>
        <div class="article-readmore"><a href="/k8s/controller-manager-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/api-server-source-read/" class="article-titles">
          <h2 class="article-title">K8S Api Server 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 13, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  Api Server架构 kubernetes api server 提供了增删改查，list,watch各类资源的接口，对请求进行验证，鉴权及admission controller, 请求成功后将结果更新到后端存储etcd（也可以是其他存储）中。
api server 架构从上到下可以分为以下几层：
1.Api层，主要包含三种api core api: 主要在/api/v1下
分组api: 其path为/apis/$NAME/$VERSION
暴露系统状态的api: 如/metrics,/healthz,/logs,/openapi等
$ curl -k https://127.0.0.1:56521 { &#34;paths&#34;: [ &#34;/api&#34;, &#34;/api/v1&#34;, &#34;/apis&#34;, &#34;/apis/&#34;, &#34;/apis/admissionregistration.k8s.io&#34;, &#34;/apis/admissionregistration.k8s.io/v1beta1&#34;, ... &#34;/healthz&#34;, &#34;/healthz/autoregister-completion&#34;, &#34;/healthz/etcd&#34;, ... &#34;/logs&#34;, &#34;/metrics&#34;, &#34;/openapi/v2&#34;, &#34;/swagger-2.0.0.json&#34;, &#34;/swaggerapi&#34;, &#34;/version&#34; 2.访问控制层 当用户访问api接口时，访问控制层将会做如下校验：
对用户进行身份校验(authentication)&ndash;&gt; 校验用户对k8s资源对象访问权限(authorization) &ndash;&gt; 根据配置的资源访问许可逻辑(admission control)，判断是否访问。
下面分别对这三种校验方式进行说明：
2.1 Authentication 身份校验 k8s提供了三种客户端身份校验方式：https双向证书认证，http token方式认证， http base 认证也就是用户名密码方式
a. 基于CA根证书签名的双向数字证书认证
client和server向CA机构申请证书
client-&gt;server，server下发服务端证书，client利用证书验证server是否合法
client发送客户端证书给server，server利用证书校验client是否合法
client和server利用随机密钥加密消息，然后通信
apiserver启动的时候通过&ndash;client-ca-file=XXXX配置签发client证书的CA，当client发送证书过来时，apiserver使用CA验证，如果证书合法，则可进行通信。
这种方式的优点是安全，缺点是无法撤销用户证书，创建集群就绑定了证书。
$ k get pods kube-apiserver-kind-control-plane -n kube-system -o yaml - command: - kube-apiserver - --authorization-mode=Node,RBAC - --advertise-address=172.17.0.2 - --allow-privileged=true - --client-ca-file=/etc/kubernetes/pki/ca.crt b.
        </div>
        <div class="article-readmore"><a href="/k8s/api-server-source-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
    </section>
    

<ul class="pagination">
    
    <li class="page-item">
        <a href="/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
    </li>
    
    <li class="page-item disabled">
    <a  class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
    </li>
    
    
    
    
    
    
    
        
        
    
    
    <li class="page-item active"><a class="page-link" href="/">1</a></li>
    
    
    
    
    
    
        
        
    
    
    <li class="page-item"><a class="page-link" href="/page/2/">2</a></li>
    
    
    <li class="page-item">
    <a href="/page/2/" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
    </li>
    
    <li class="page-item">
        <a href="/page/2/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
    </li>
    
</ul>


  </div>



<div class="site-footer">
  <div class="copyright">&copy; Copyright 2017 Your name</div>
  <ul class="site-footer-items">
    <li class="site-footer-item-rsslink">
      <a href="/index.xml" type="application/rss+xml" target="_blank" title="RSS">
        <i class="fas fa-rss"></i>
      </a>
    </li>
    <li class="site-footer-item-about"><a href="/about/" title="About">About</a></li>
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://github.com/taikii/whiteplain">Whiteplain</a>
  </div>
</div>


</body>
</html>
