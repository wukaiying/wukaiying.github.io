<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kaiying</title>
    <link>https://wukaiying.github.io/</link>
    <description>Recent content on Kaiying</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Copyright 2017 Your name</copyright>
    <lastBuildDate>Tue, 19 May 2020 11:49:28 +0800</lastBuildDate>
    
	<atom:link href="https://wukaiying.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>golang 实现加载动态库</title>
      <link>https://wukaiying.github.io/golang/go-plugin/</link>
      <pubDate>Thu, 27 Feb 2020 14:13:43 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/golang/go-plugin/</guid>
      <description>golang开发中需要用到插件动态加载技术，需要主程序动态加载一些功能模块，不需要修改主程序。相比于java ,java 可以通过加载class文件方式来实现模块加载，而golang通常都是单个二进制文件。 go-plugin则使用另外一种方式实现插件加载，主进程和插件进程是两个独立的进程，二者通过rpc/grpc的方式进行通信， 从而实现主进程调用插件进程。
其次，golang也原生提供一种插件模块加载机制plugins，下面依次进行介绍。
go-plugin 项目地址https://github.com/hashicorp/go-plugin
特征  插件是Go接口的实现：这让插件的编写、使用非常自然。对于插件的作者来说，他只需要实现一个Go接口即可；对于插件的用户来说，他只需要调用一个Go接口即可。go-plugin会处理好本地调用转换为gRPC调用的所有细节 跨语言支持：插件可以基于任何主流语言编写，同样可以被任何主流语言消费 支持复杂的参数、返回值：go-plugin可以处理接口、io.Reader/Writer等复杂类型 双向通信：为了支持复杂参数，宿主进程能够将接口实现发送给插件，插件也能够回调到宿主进程 内置日志系统：任何使用log标准库的的插件，都会将日志信息传回宿主机进程。宿主进程会在这些日志前面加上插件二进制文件的路径，并且打印日志 协议版本化：支持一个简单的协议版本化，增加版本号后可以基于老版本协议的插件无效化。当接口签名变化时应当增加版本 标准输出/错误同步：插件以子进程的方式运行，这些子进程可以自由的使用标准输出/错误，并且打印的内容会被自动同步到宿主进程，宿主进程可以为同步的日志指定一个io.Writer TTY Preservation：插件子进程可以链接到宿主进程的stdin文件描述符，以便要求TTY的软件能正常工作 宿主进程升级：宿主进程升级的时候，插件子进程可以继续允许，并在升级后自动关联到新的宿主进程 加密通信：gRPC信道可以加密 完整性校验：支持对插件的二进制文件进行Checksum 插件崩溃了，不会导致宿主进程崩溃 容易安装：只需要将插件放到某个宿主进程能够访问的目录即可  用法 这里我们通过一个实例来体验一下go-plugin插件开发。我们需要完成的功能是，主程序暴露了两个方法，插件模块去实现这两个方法，然后主程序加载插件完成功能实现。 最终达成的效果是通过Put方法设置一个key/value对，通过get方法获取key对应的值。
Put(key string, value []byte) error Get(key string) ([]byte, error) ([]byte, error) 新建项目 这里我先将完整项目目录结构列出来
. ├── Makefile ├── cmd │ ├── cmd │ ├── kv_wky │ └── main.go ├── pkg │ ├── plugins │ │ └── proto │ │ ├── plugin.pb.go │ │ └── plugin.proto │ └── shared │ ├── grpc.go │ └── plugin.go └── pluginclient ├── main.go └── pluginclient 定义proto 新建pkg/plugins/proto/plugin.proto文件，并生成plugin.pb.go
syntax = &amp;#34;proto3&amp;#34;;package proto;message GetRequest { string key = 1;}message GetResponse { bytes value = 1;}message PutRequest { string key = 1; bytes value = 2;}message Empty {}service KV { rpc Get(GetRequest) returns (GetResponse); rpc Put(PutRequest) returns (Empty);} 定义对外暴露的插件接口 新建pkg/shared/plugin.</description>
    </item>
    
    <item>
      <title>Ceph学习笔记</title>
      <link>https://wukaiying.github.io/k8sstorage/ceph/</link>
      <pubDate>Tue, 19 May 2020 11:49:28 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8sstorage/ceph/</guid>
      <description>$\color{#4285f4}{介绍}\color ceph 独一无二地用统一的系统提供了三大存储方式：
ceph 块存储
ceph 文件存储(cephFS)
ceph 对象存储
快速部署 ceph部署方式有很多中，常用方式有使用ceph-deploy来进行部署，这里为了测试使用docker方式快速搭建出一套ceph单机环境出来
docker 部署 $ rm -fr /etc/ceph $ rm -fr /var/lib/ceph $ mkdir -p /etc/ceph $ mkdir -p /var/lib/ceph $ docker run -d --net=host --privileged=true -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -e MON_IP=192.168.0.2 -e CEPH_PUBLIC_NETWORK=192.168.0.1/23 -e CEPH_DEMO_UID=qqq -e CEPH_DEMO_ACCESS_KEY=qqq -e CEPH_DEMO_SECRET_KEY=qqq -e CEPH_DEMO_BUCKET=qqq --name cephdemo ceph/daemon demo $ curl http://192.168.0.0:5000 $ docker exec -it cephdemo ceph -s 架构及组件   RADOS (Reliable Automomic Distributed Object Store)是ceph集群的基础，ceph中一切都是对象的形式存储，RADOS负责存储这些对象。确保数据一致性和可靠性。 ceph数据访问方法(如RBD,CephFS,RADOSGW,librados)所有操作方法都是在RADOS之上构建的。
  OSD ceph对象存储设备，负责存储用户实际数据，一个OSD守护进程和一个物理磁盘绑定，物理磁盘的数量和OSD守护进程的数量是一致的。 ceph osd由一个已经存在的linux文件系统物理磁盘驱动器和OSD服务组成。
 ceph OSD 文件系统 --&amp;gt;btrfs或XFS或ext4 物理磁盘    通过命令查看单个节点上OSD的状态：
$ service ceph status osd #查看所有osd的ID $ service osd ls #检查osd map的状态 $ ceph osd stat #检查osd 树形图 $ ceph osd tree  MON Ceph monitor 跟踪整个集群的健康状态 一个典型的ceph集群通常包含多个monitor节点，且节点数为奇数个。使用Paxos算法来实现数据一致性。  $ service ceph status mon $ ceph mon stat $ ceph mon_status $ ceph mon dump   librados 提供了访问RADOS的接口，支持PHP,Ruby,Java,Python等语言</description>
    </item>
    
    <item>
      <title>Calico 学习及源码分析</title>
      <link>https://wukaiying.github.io/k8s/calico-source-code/</link>
      <pubDate>Thu, 07 May 2020 11:38:20 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/calico-source-code/</guid>
      <description>背景 如何选取合适的网络插件：
1.集群环境因素
如果集群节点机器为虚拟机，不允许机器之间直接二层互联（所谓二层互联也就是说通过交换机可以直接通信，而可以通过交换机直接通信则说明通过交换机互联的机器处于同一个子网中，不同子网直接通信则需要用到路由器）， 必须要带有IP地址这种三层的内容才能去做转发，或者限制机器只能使用某些IP等约束，这种情况只能使用overlay模式去实现。常见的有Flannel-vxlan,Calico-ipip,Weave等。
物理机环境同一个子网底层网络可以通过交换机直接通信，此时我们可以使用underlay模式或者路由模式的网络插件。这样避免了使用vxlan封包解包d导致性能降低，常见的插件有Flannel-hostgw,Calico-bgp等。
2.性能因素
pod 创建速度，在pod需要扩容时，overlay模式创建pod速度快，内核接口就可以完成这些操作。underlay模式需要准备底层网络资源，速度较慢。
其次，overlay模式包含封包解包，会带来一些包头的损失、CPU 的消耗等，对网络性能的要求比较高，比如说机器学习、大数据这些场景就不适合使用 Overlay 模式。
calico calico网络模型主要的组件
 Felix:  每一个节点上的agent进程，监听etcd，当有新节点加入或者新的容器创建，felix为它设置网卡，IP,MAC，然后更新节点路由表，添加策略到Iptables
  etcd: 数据存储
  BGP Client
  每一个节点上都有BGP Client，监听felix写入的路由信息，然后通过BGP协议广播到其他Host节点上去，实现网络互通。该模式比较适合集群规模较小的场景，因为每一个BGP Client需要和集群其他节点互连。集群规模较大时，路由表会被撑满。
 BGP Route Reflector:  针对BGP Client存在的问题，对于大规模集群，可以采用BGP的Router Reflector的方法，使所有BGP Client仅与特定Router Reflector节点互联并做路由同步，从而大大减少连接数。 calico 网络模式 calico 作为容器网络方案最大的特点就是它提供了纯三层网络报文转发方案，也就是说每个节点中的容器都可以通过IP地址来直接通信，中间可以通过路由转发找到对方。对比于flannel-hostgw方案，虽然也是通过路由实现通信，但是它依赖于集群所有节点二层连通，然后通过更新每个节点路由的方式实现通信，也就是说flannel-hostgw 方案需要所有节点在同一个子网中，不在同一个子网无法通信。
calico可以实现跨子网通信，是因为他在每一个集群节点中加入了虚拟路由(vrouter)，通过BGP路由协议，使得每一个节点中的路由信息可以被传播到其他路由设备，实现三层网络通信。 �������������������������������������������� ���������������������������������������� ��������������������������������������� ��������������������������������������� ��������������������������������� 但是，并不是所有的网络都支持BGP路由协议，calico也提供了overlay模式解决方案&amp;mdash;ipip模式
IPIP模式 和其他overlay模式实现的原理一样，ipip模式是在各个节点之间架起一个隧道，使用节点IP封装原始IP报文，启用ipip模式是，calico会在每一个节点上创建一个名字为tunl0的虚拟网络接口。
ping数据包流向图
我们来分析一下一个集群中处于两个不同node的pod数据包是如何传输的，集群网络模式是calico-ipip
node2节点ip为172.31.127.252，node2中pod2的ip为10.100.9.206
node1节点ip为172.31.112.2，node1中pod1的ip为10.100.15.150，如下图所示：
 pod网络分析  首先进入pod1
$ kubectl exec -it pod1 /bin/bash 查看pod1中包含的网络设备，可以看到pod1中只包含lo和eth0
$ ip addr 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 4: eth0@if26: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1440 qdisc noqueue state UP group default link/ether ce:83:2b:89:af:9e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.</description>
    </item>
    
    <item>
      <title>K8S Proxy 源码分析</title>
      <link>https://wukaiying.github.io/k8s/kube-proxy-source-code-read/</link>
      <pubDate>Wed, 29 Apr 2020 10:15:18 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/kube-proxy-source-code-read/</guid>
      <description>前言 在分析kube-proxy之前，我们需要先知道k8s中另外一个概念service。
service 是一组具有相同 label pod 集合的抽象，集群内外的各个服务可以通过 service 进行互相访问。
当创建一个 service 对象时也会对应创建一个 endpoint 对象，endpoint 存放的是pod服务的ip地址，service 只是将多个pod进行关联起来。
当用户通过service访问后端pod服务时，实际的路由转发都是由 kubernetes 中的 kube-proxy 组件来实现，因此，service 必须结合 kube-proxy 使用。
kube-proxy 组件可以运行在 kubernetes 集群中的每一个节点上也可以只运行在单独的几个节点上，其会根据 service 和 endpoints 的变动来刷新节点上 iptables 或者 ipvs 中保存的路由规则。
service 工作原理 endpoint controller 负责监听 service 和对应 pod 的变化，更新对应 service 的 endpoints 对象。当用户创建 service 后 endpoints controller 会监听 pod 的状态，当 pod 处于 running 且准备就绪时，endpoints controller 会将 pod ip 记录到 endpoints 对象中。 kube-proxy 会监听 service 和 endpoints 的变化并调用其代理模块在主机上刷新路由规则。
总结就是，endpoint controller负责更新service ip和其对应后端服务endpoint ip的关系。而kube-proxy则负责更新各个节点的iptables或者ipvs使得流量能够被正常转发。
service 暴露服务方式 ClusterIp ClusterIP 类型的 service 是 kubernetes 集群默认的服务暴露方式，它只能用于集群内部通信，可以被各 pod 访问，其访问方式为：
pod ---&amp;gt; ClusterIP:ServicePort --&amp;gt; (iptables)DNAT --&amp;gt; PodIP:containePort NodePort client ---&amp;gt; NodeIP:NodePort ---&amp;gt; ClusterIP:ServicePort ---&amp;gt; (iptables)DNAT ---&amp;gt; PodIP:containePort LoadBalancer Ingress service 数据包流动分析 实验背景就是，我们启动三个nginx pod，pod3和pod2在node2中，而pod1在node1中。网络设备结构图如下： pod2和pod3在同一个node中，为了简洁暂未画出。</description>
    </item>
    
    <item>
      <title>Kubelet 创建pod流程分析</title>
      <link>https://wukaiying.github.io/k8s/kubelet-create-pod-lifecycle/</link>
      <pubDate>Tue, 28 Apr 2020 10:38:28 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/kubelet-create-pod-lifecycle/</guid>
      <description>前言 上一节我们分析了kubelet启动的基本流程，这一节我们分析一下当有新的pod分配到当前节点时，kubelet如何创建一个pod
syncLoop 上一节最后我们分析到kl.syncLoop(updates, kl)，是kubelet里面一个主循环逻辑，看一下它的具体实现： 它从不同的管道（文件、URL 和 apiserver）监听变化，并把它们汇聚起来。 当有新的变化发生时，它会调用对应的处理函数，保证 pod 处于期望的状态。 如果 pod 没有变化，它也会定期保证所有的容器和最新的期望状态保持一致。这个方法是 for 循环，不会退出。
func (kl *Kubelet) syncLoop(updates &amp;lt;-chan kubetypes.PodUpdate, handler SyncHandler) { klog.Info(&amp;#34;Starting kubelet main sync loop.&amp;#34;) syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if err := kl.runtimeState.runtimeErrors(); err != nil { klog.Infof(&amp;#34;skipping pod synchronization - %v&amp;#34;, err) // exponential backoff 	time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } // reset backoff if we have a success 	duration = base kl.</description>
    </item>
    
    <item>
      <title>Kube Scheduler 优选策略源码分析</title>
      <link>https://wukaiying.github.io/k8s/kube-scheduler-priority-source-read/</link>
      <pubDate>Fri, 24 Apr 2020 15:32:13 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/kube-scheduler-priority-source-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 预选阶段完成后，进入优选阶段，将需要调度的Pod列表和Node列表传入各种优选算法进行打分，最终整合成结果集HostPriorityList
//pkg/scheduler/core/generic_scheduler.go:189 // Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) { ... trace.Step(&amp;#34;Basic checks done&amp;#34;) startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod) if err != nil { return result, err } // Run &amp;#34;postfilter&amp;#34; plugins. 	postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses) if !</description>
    </item>
    
    <item>
      <title>Kubelet 架构分析</title>
      <link>https://wukaiying.github.io/k8s/kubelet-structure/</link>
      <pubDate>Thu, 23 Apr 2020 14:52:56 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/kubelet-structure/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kubelet是k8s集群中节点的代理人，通过watch api server，来处理发送给本节点的任务，看一下官方对kubelet的描述：
The kubelet is the primary &amp;quot;node agent&amp;quot; that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy.</description>
    </item>
    
    <item>
      <title>K8S Scheduler 源码分析</title>
      <link>https://wukaiying.github.io/k8s/kube-scheduler-source-code-read/</link>
      <pubDate>Wed, 22 Apr 2020 13:56:37 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/kube-scheduler-source-code-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kube-scheduler 是master节点中重要的组件之一，它同watch接口监听api server中需要调度的pod，然后通过其预选，优选算法来确定pod需要被调度到哪个节点上去。
看一下官方对scheduler 调度流程的描述：The Kubernetes Scheduler
主要步骤如下：
 首先通过预选阶段过滤掉不合适的节点，例如根据podSpec中对系统资源的需求，会计算出所有节点剩余的资源是否可以满足pod需求，如果不满足则该节点会被抛弃。 其次通过预选阶段的节点，会再次进行优选。比如在满足pod资源需求的基础上，会优选出当前负载最小节点，保证每个集群节点资源被均衡使用。 最后选择打分最高的节点作为目标节点，如果得分相同则会随机选择一个，进行调度。  For given pod: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Pred. filters: node 3 doesn&#39;t have enough resource +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | remaining nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Priority function: node 1: p=2 node 2: p=5 +-------------------+-------------------------+ | | v select max{node priority} = node 2 入口 scheduler使用cobra创建命令行客户端，进入NewSchedulerCommand</description>
    </item>
    
    <item>
      <title>Replica Set Controller 源码分析</title>
      <link>https://wukaiying.github.io/k8s/replicaset-source-code-read/</link>
      <pubDate>Tue, 21 Apr 2020 15:53:20 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/replicaset-source-code-read/</guid>
      <description>版本环境 kubernetes版本：kubernetes:v1.16.0
go环境：go version go1.13.4 darwin/amd64
前言 前面文章已经总结到，deployment controller通过replicaset来控制pod的创建和删除，进而实现更高级操作例如，滚动更新，回滚等。本文继续从源码角度 分析replicaset controller如何控制pod的创建和删除。
在分析源码前先考虑一下 replicaset 的使用场景，在平时的操作中其实我们并不会直接操作 replicaset，replicaset 也仅有几个简单的操作，创建、删除、更新等。
源码分析 入口 我们在上一篇文章中也说道controller manager是所有k8s 资源controller的管控中心，包含的主要逻辑就是高可用选择leader，然后 遍历所有的controllers,对每一个controller进行初始化并启动，也就是说只要集群启动，所有的controller也会都启动起来，开始list-watch api server，来准备创建资源。
cmd/kube-controller-manager/app/controllermanager.go:386 func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[&amp;#34;endpoint&amp;#34;] = startEndpointController controllers[&amp;#34;endpointslice&amp;#34;] = startEndpointSliceController ... controllers[&amp;#34;deployment&amp;#34;] = startDeploymentController controllers[&amp;#34;replicaset&amp;#34;] = startReplicaSetController } 接下来进入startReplicaSetController, 发现通过NewReplicaSetController初始化了ReplicaSetController，并并调用run方法启动controller。
cmd/kube-controller-manager/app/apps.go:69 func startReplicaSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: &amp;#34;apps&amp;#34;, Version: &amp;#34;v1&amp;#34;, Resource: &amp;#34;replicasets&amp;#34;}] { return nil, false, nil } go replicaset.NewReplicaSetController( ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(&amp;#34;replicaset-controller&amp;#34;), replicaset.BurstReplicas, ).Run(int(ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs), ctx.Stop) return nil, true, nil } 我们先分析NewReplicaSetController看一下初始化ReplicaSetController的具体步骤：发现他会监听rs的add,update,delete操作，和pod的add,update,delete操作。 当这两种资源发送变化时，将key(name+namespace)添加到queue队列中去。
//pkg/controller/replicaset/replica_set.go:126  func NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.</description>
    </item>
    
    <item>
      <title>K8S Controller Manager 源码分析</title>
      <link>https://wukaiying.github.io/k8s/controller-manager-source-code-read/</link>
      <pubDate>Sun, 19 Apr 2020 21:08:44 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/controller-manager-source-code-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 Controller Manager内部包含Replication Controller、Node Controller、 ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Service Controller及Endpoint Controller这8种Controller， 每个Controller，它们通过API Server提供的（List-Watch）接口实时监控集群中特定资源的状态变化，当发生各种故障 导致某资源对象的状态发生变化时，Controller会尝试将其状态调整为期望的状态。
代码总体解析 入口 # cmd/kube-controller-manager/controller-manager.go func main() { rand.Seed(time.Now().UnixNano()) command := app.NewControllerManagerCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 进入NewControllerManagerCommand 函数，可以看到新建ControllerManager需要两步 启动参数配置，及Run函数，进入Run函数。
func NewControllerManagerCommand() *cobra.Command { s, err := options.NewKubeControllerManagerOptions() if err != nil { klog.Fatalf(&amp;#34;unable to initialize command options: %v&amp;#34;, err) } cmd := &amp;amp;cobra.Command{ Use: &amp;#34;kube-controller-manager&amp;#34;, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, &amp;#34;%v\n&amp;#34;, err) os.</description>
    </item>
    
    <item>
      <title>K8S Api Server 源码分析</title>
      <link>https://wukaiying.github.io/k8s/api-server-source-read/</link>
      <pubDate>Mon, 13 Apr 2020 20:31:49 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/api-server-source-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  Api Server架构 kubernetes api server 提供了增删改查，list,watch各类资源的接口，对请求进行验证，鉴权及admission controller, 请求成功后将结果更新到后端存储etcd（也可以是其他存储）中。
api server 架构从上到下可以分为以下几层：
1.Api层，主要包含三种api core api: 主要在/api/v1下
分组api: 其path为/apis/$NAME/$VERSION
暴露系统状态的api: 如/metrics,/healthz,/logs,/openapi等
$ curl -k https://127.0.0.1:56521 { &amp;#34;paths&amp;#34;: [ &amp;#34;/api&amp;#34;, &amp;#34;/api/v1&amp;#34;, &amp;#34;/apis&amp;#34;, &amp;#34;/apis/&amp;#34;, &amp;#34;/apis/admissionregistration.k8s.io&amp;#34;, &amp;#34;/apis/admissionregistration.k8s.io/v1beta1&amp;#34;, ... &amp;#34;/healthz&amp;#34;, &amp;#34;/healthz/autoregister-completion&amp;#34;, &amp;#34;/healthz/etcd&amp;#34;, ... &amp;#34;/logs&amp;#34;, &amp;#34;/metrics&amp;#34;, &amp;#34;/openapi/v2&amp;#34;, &amp;#34;/swagger-2.0.0.json&amp;#34;, &amp;#34;/swaggerapi&amp;#34;, &amp;#34;/version&amp;#34; 2.访问控制层 当用户访问api接口时，访问控制层将会做如下校验：
对用户进行身份校验(authentication)&amp;ndash;&amp;gt; 校验用户对k8s资源对象访问权限(authorization) &amp;ndash;&amp;gt; 根据配置的资源访问许可逻辑(admission control)，判断是否访问。
下面分别对这三种校验方式进行说明：
2.1 Authentication 身份校验 k8s提供了三种客户端身份校验方式：https双向证书认证，http token方式认证， http base 认证也就是用户名密码方式
a. 基于CA根证书签名的双向数字证书认证
client和server向CA机构申请证书
client-&amp;gt;server，server下发服务端证书，client利用证书验证server是否合法
client发送客户端证书给server，server利用证书校验client是否合法
client和server利用随机密钥加密消息，然后通信
apiserver启动的时候通过&amp;ndash;client-ca-file=XXXX配置签发client证书的CA，当client发送证书过来时，apiserver使用CA验证，如果证书合法，则可进行通信。
这种方式的优点是安全，缺点是无法撤销用户证书，创建集群就绑定了证书。
$ k get pods kube-apiserver-kind-control-plane -n kube-system -o yaml - command: - kube-apiserver - --authorization-mode=Node,RBAC - --advertise-address=172.17.0.2 - --allow-privileged=true - --client-ca-file=/etc/kubernetes/pki/ca.crt b.</description>
    </item>
    
    <item>
      <title>CoreDNS生产案：pod出现dns解析大量失败的问题</title>
      <link>https://wukaiying.github.io/k8s/dns-problem/</link>
      <pubDate>Tue, 31 Mar 2020 17:26:43 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/dns-problem/</guid>
      <description>问题 收到阿里云K8S集群监控告警：
CoreDNS 5分钟内NXDOMAIN响应百分比大于50% k8s.coredns.response[aliyun,172.22.82.25:9153,NXDOMAIN]
是coredns的172.22.82.25的这个pod出现dns解析大量失败的问题。（解析成功的日志是NOERROR，解析失败是：NXDMAIN）
分析 首先是查看这个coredns pod的日志：
[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl get pods -n kube-system -o wide | grep coredns |grep 172.22.82.25 coredns-57dc86754b-9schh 1/1 Running 0 17d 172.22.82.25 cn-hangzhou.xxx.xxx.xxx.xxx &amp;lt;none&amp;gt;[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl logs -n kube-system coredns-57dc86754b-9schh | tail -10 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:54106 - 24311 &amp;quot;A IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&amp;quot; NOERROR qr,rd,ra 190 0.00003395s 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:56730 - 20278 &amp;quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&amp;quot; NOERROR qr,rd,ra 232 0.000072002s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:41047 - 53211 &amp;quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&amp;quot; NXDOMAIN qr,rd,ra 173 0.00004196s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:40581 - 40714 &amp;quot;A IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&amp;quot; NXDOMAIN qr,rd,ra 173 0.</description>
    </item>
    
    <item>
      <title>My Second</title>
      <link>https://wukaiying.github.io/post/my-second/</link>
      <pubDate>Tue, 11 Feb 2020 11:14:48 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/post/my-second/</guid>
      <description></description>
    </item>
    
    <item>
      <title>hugo&#43;gitpage搭建个人博客</title>
      <link>https://wukaiying.github.io/post/first/</link>
      <pubDate>Tue, 11 Feb 2020 10:01:25 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/post/first/</guid>
      <description>安装hugo brew install hugo hugo version 生成blog根目录 hugo new site blog config.toml 是配置文件，在里面可以定义博客地址、构建配置、标题、导航栏等等。
themes 是主题目录，可以去 themes.gohugo.io 下载喜欢的主题。
content 是博客文章的目录。
安装主题 https://themes.gohugo.io/whiteplain/
使用hugo写文章 hugo new post/my-first-post.md 设置文章相关属性
title: &amp;quot;My First Post&amp;quot; date: 2017-12-14T11:18:15+08:00 weight: 70 markup: mmark draft: false keywords: [&amp;quot;hugo&amp;quot;] description: &amp;quot;第一篇文章&amp;quot; tags: [&amp;quot;hugo&amp;quot;, &amp;quot;pages&amp;quot;] categories: [&amp;quot;pages&amp;quot;] author: &amp;quot;&amp;quot; 预览 hugo server -D 确保本地网站正常，hugo server运行后在本地打开localhost:1313检查网站效果和内容，注意hugo server这个命令不会构建草稿，所以如果有草稿需要发布，将文章中的draft设置为false。
将hugo部署在gitpage 在Github创建一个仓库，例如名字叫blog，可以是私有的，这个仓库用来存放网站内容和源文件。
再创建一个名称为.github.io的仓库，username为GitHub用户名，这个仓库用于存放最终发布的网站内容。
进入本地网站目录，将本地网站全部内容推送到远程blog仓库。
cd blog git init git remote add origin git@github.com:wukaiying/blog.git git add -A git commit -m &amp;quot;first commit&amp;quot; git push -u origin master 删除本地网站目录下的public文件夹
创建public子模块
git submodule add -b master git@github.com:wukaiying/wukaiying.github.io.git public 然后就可以执行hugo命令，此命令会自动将网站静态内容生成到public文件夹，然后提交到远程blog仓库
cd public git status git add . git commit -m &amp;quot;first commit&amp;quot; git push -u orgin master 过一会就可以打开.</description>
    </item>
    
    <item>
      <title>Kubernetes中leaderelection实现组件高可用</title>
      <link>https://wukaiying.github.io/k8s/my-first-post/</link>
      <pubDate>Tue, 11 Feb 2020 10:01:25 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/my-first-post/</guid>
      <description>在Kubernetes中，通常kube-schduler和kube-controller-manager都是多副本进行部署的来保证高可用，而真正在工作的实例其实只有一个。
这里就利用到 leaderelection 的选主机制，保证leader是处于工作状态，并且在leader挂掉之后，从其他节点选取新的leader保证组件正常工作。今天就来看看这个包的使用以及它内部是如何实现的。
基本使用 以下是一个简单使用的例子，编译完成之后同时启动多个进程，但是只有一个进程在工作，当把leader进程kill掉之后，会重新选举出一个leader进行工作，即执行其中的 run 方法：
/* 例子来源于client-go中的example包中， */ package main import ( &amp;#34;context&amp;#34; &amp;#34;flag&amp;#34; &amp;#34;os&amp;#34; &amp;#34;os/signal&amp;#34; &amp;#34;syscall&amp;#34; &amp;#34;time&amp;#34; &amp;#34;github.com/google/uuid&amp;#34; metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34; clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34; &amp;#34;k8s.io/client-go/rest&amp;#34; &amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34; &amp;#34;k8s.io/client-go/tools/leaderelection&amp;#34; &amp;#34;k8s.io/client-go/tools/leaderelection/resourcelock&amp;#34; &amp;#34;k8s.io/klog&amp;#34; ) func buildConfig(kubeconfig string) (*rest.Config, error) { if kubeconfig != &amp;#34;&amp;#34; { cfg, err := clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, kubeconfig) if err != nil { returnnil, err } return cfg, nil } cfg, err := rest.InClusterConfig() if err != nil { returnnil, err } return cfg, nil } func main() { klog.InitFlags(nil) var kubeconfig string var leaseLockName string var leaseLockNamespace string var id string flag.StringVar(&amp;amp;kubeconfig, &amp;#34;kubeconfig&amp;#34;, &amp;#34;&amp;#34;, &amp;#34;absolute path to the kubeconfig file&amp;#34;) flag.</description>
    </item>
    
  </channel>
</rss>