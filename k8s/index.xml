<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on Kaiying</title>
    <link>https://wukaiying.github.io/k8s/</link>
    <description>Recent content in K8s on Kaiying</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Copyright 2017 Your name</copyright>
    <lastBuildDate>Tue, 21 Apr 2020 15:53:20 +0800</lastBuildDate>
    
	<atom:link href="https://wukaiying.github.io/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Replica Set Controller 源码分析</title>
      <link>https://wukaiying.github.io/k8s/replicaset-source-code-read/</link>
      <pubDate>Tue, 21 Apr 2020 15:53:20 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/replicaset-source-code-read/</guid>
      <description>版本环境 kubernetes版本：kubernetes:v1.16.0
go环境：go version go1.13.4 darwin/amd64
前言 </description>
    </item>
    
    <item>
      <title>K8S Controller Manager 源码分析</title>
      <link>https://wukaiying.github.io/k8s/controller-manager-source-code-read/</link>
      <pubDate>Sun, 19 Apr 2020 21:08:44 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/controller-manager-source-code-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 Controller Manager内部包含Replication Controller、Node Controller、 ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Service Controller及Endpoint Controller这8种Controller， 每个Controller，它们通过API Server提供的（List-Watch）接口实时监控集群中特定资源的状态变化，当发生各种故障 导致某资源对象的状态发生变化时，Controller会尝试将其状态调整为期望的状态。
代码总体解析 入口 # cmd/kube-controller-manager/controller-manager.go func main() { rand.Seed(time.Now().UnixNano()) command := app.NewControllerManagerCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 进入NewControllerManagerCommand 函数，可以看到新建ControllerManager需要两步 启动参数配置，及Run函数，进入Run函数。
func NewControllerManagerCommand() *cobra.Command { s, err := options.NewKubeControllerManagerOptions() if err != nil { klog.Fatalf(&amp;#34;unable to initialize command options: %v&amp;#34;, err) } cmd := &amp;amp;cobra.Command{ Use: &amp;#34;kube-controller-manager&amp;#34;, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, &amp;#34;%v\n&amp;#34;, err) os.</description>
    </item>
    
    <item>
      <title>K8S Api Server 源码分析</title>
      <link>https://wukaiying.github.io/k8s/api-server-source-read/</link>
      <pubDate>Mon, 13 Apr 2020 20:31:49 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/api-server-source-read/</guid>
      <description>版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  Api Server架构 kubernetes api server 提供了增删改查，list,watch各类资源的接口，对请求进行验证，鉴权及admission controller, 请求成功后将结果更新到后端存储etcd（也可以是其他存储）中。
api server 架构从上到下可以分为以下几层：
1.Api层，主要包含三种api core api: 主要在/api/v1下
分组api: 其path为/apis/$NAME/$VERSION
暴露系统状态的api: 如/metrics,/healthz,/logs,/openapi等
$ curl -k https://127.0.0.1:56521 { &amp;#34;paths&amp;#34;: [ &amp;#34;/api&amp;#34;, &amp;#34;/api/v1&amp;#34;, &amp;#34;/apis&amp;#34;, &amp;#34;/apis/&amp;#34;, &amp;#34;/apis/admissionregistration.k8s.io&amp;#34;, &amp;#34;/apis/admissionregistration.k8s.io/v1beta1&amp;#34;, ... &amp;#34;/healthz&amp;#34;, &amp;#34;/healthz/autoregister-completion&amp;#34;, &amp;#34;/healthz/etcd&amp;#34;, ... &amp;#34;/logs&amp;#34;, &amp;#34;/metrics&amp;#34;, &amp;#34;/openapi/v2&amp;#34;, &amp;#34;/swagger-2.0.0.json&amp;#34;, &amp;#34;/swaggerapi&amp;#34;, &amp;#34;/version&amp;#34; 2.访问控制层 当用户访问api接口时，访问控制层将会做如下校验：
对用户进行身份校验(authentication)&amp;ndash;&amp;gt; 校验用户对k8s资源对象访问权限(authorization) &amp;ndash;&amp;gt; 根据配置的资源访问许可逻辑(admission control)，判断是否访问。
下面分别对这三种校验方式进行说明：
2.1 Authentication 身份校验 k8s提供了三种客户端身份校验方式：https双向证书认证，http token方式认证， http base 认证也就是用户名密码方式
a. 基于CA根证书签名的双向数字证书认证
client和server向CA机构申请证书
client-&amp;gt;server，server下发服务端证书，client利用证书验证server是否合法
client发送客户端证书给server，server利用证书校验client是否合法
client和server利用随机密钥加密消息，然后通信
apiserver启动的时候通过&amp;ndash;client-ca-file=XXXX配置签发client证书的CA，当client发送证书过来时，apiserver使用CA验证，如果证书合法，则可进行通信。
这种方式的优点是安全，缺点是无法撤销用户证书，创建集群就绑定了证书。
$ k get pods kube-apiserver-kind-control-plane -n kube-system -o yaml - command: - kube-apiserver - --authorization-mode=Node,RBAC - --advertise-address=172.17.0.2 - --allow-privileged=true - --client-ca-file=/etc/kubernetes/pki/ca.crt b.</description>
    </item>
    
    <item>
      <title>CoreDNS生产案：pod出现dns解析大量失败的问题</title>
      <link>https://wukaiying.github.io/k8s/dns-problem/</link>
      <pubDate>Tue, 31 Mar 2020 17:26:43 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/dns-problem/</guid>
      <description>问题 收到阿里云K8S集群监控告警：
CoreDNS 5分钟内NXDOMAIN响应百分比大于50% k8s.coredns.response[aliyun,172.22.82.25:9153,NXDOMAIN]
是coredns的172.22.82.25的这个pod出现dns解析大量失败的问题。（解析成功的日志是NOERROR，解析失败是：NXDMAIN）
分析 首先是查看这个coredns pod的日志：
[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl get pods -n kube-system -o wide | grep coredns |grep 172.22.82.25 coredns-57dc86754b-9schh 1/1 Running 0 17d 172.22.82.25 cn-hangzhou.xxx.xxx.xxx.xxx &amp;lt;none&amp;gt;[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl logs -n kube-system coredns-57dc86754b-9schh | tail -10 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:54106 - 24311 &amp;quot;A IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&amp;quot; NOERROR qr,rd,ra 190 0.00003395s 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:56730 - 20278 &amp;quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&amp;quot; NOERROR qr,rd,ra 232 0.000072002s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:41047 - 53211 &amp;quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&amp;quot; NXDOMAIN qr,rd,ra 173 0.00004196s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:40581 - 40714 &amp;quot;A IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&amp;quot; NXDOMAIN qr,rd,ra 173 0.</description>
    </item>
    
    <item>
      <title>Kubernetes中leaderelection实现组件高可用</title>
      <link>https://wukaiying.github.io/k8s/my-first-post/</link>
      <pubDate>Tue, 11 Feb 2020 10:01:25 +0800</pubDate>
      
      <guid>https://wukaiying.github.io/k8s/my-first-post/</guid>
      <description>在Kubernetes中，通常kube-schduler和kube-controller-manager都是多副本进行部署的来保证高可用，而真正在工作的实例其实只有一个。
这里就利用到 leaderelection 的选主机制，保证leader是处于工作状态，并且在leader挂掉之后，从其他节点选取新的leader保证组件正常工作。今天就来看看这个包的使用以及它内部是如何实现的。
基本使用 以下是一个简单使用的例子，编译完成之后同时启动多个进程，但是只有一个进程在工作，当把leader进程kill掉之后，会重新选举出一个leader进行工作，即执行其中的 run 方法：
/* 例子来源于client-go中的example包中， */ package main import ( &amp;#34;context&amp;#34; &amp;#34;flag&amp;#34; &amp;#34;os&amp;#34; &amp;#34;os/signal&amp;#34; &amp;#34;syscall&amp;#34; &amp;#34;time&amp;#34; &amp;#34;github.com/google/uuid&amp;#34; metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34; clientset &amp;#34;k8s.io/client-go/kubernetes&amp;#34; &amp;#34;k8s.io/client-go/rest&amp;#34; &amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34; &amp;#34;k8s.io/client-go/tools/leaderelection&amp;#34; &amp;#34;k8s.io/client-go/tools/leaderelection/resourcelock&amp;#34; &amp;#34;k8s.io/klog&amp;#34; ) func buildConfig(kubeconfig string) (*rest.Config, error) { if kubeconfig != &amp;#34;&amp;#34; { cfg, err := clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, kubeconfig) if err != nil { returnnil, err } return cfg, nil } cfg, err := rest.InClusterConfig() if err != nil { returnnil, err } return cfg, nil } func main() { klog.InitFlags(nil) var kubeconfig string var leaseLockName string var leaseLockNamespace string var id string flag.StringVar(&amp;amp;kubeconfig, &amp;#34;kubeconfig&amp;#34;, &amp;#34;&amp;#34;, &amp;#34;absolute path to the kubeconfig file&amp;#34;) flag.</description>
    </item>
    
  </channel>
</rss>