<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>K8s - Kaiying</title>
  <meta property="og:title" content="K8s - Kaiying" />
  <meta name="twitter:title" content="K8s - Kaiying" />
  <meta name="author" content="kaiying"/>
  <meta property="og:site_name" content="Kaiying" />
  <meta property="og:url" content="https://wukaiying.github.io/k8s/" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.61.0" />
  <link href="/k8s/index.xml" rel="alternate" type="application/rss+xml" title="Kaiying" />
  <link href="/k8s/index.xml" rel="feed" type="application/rss+xml" title="Kaiying" />

  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="/js/script.js"></script>
  <script src="/js/custom.js"></script>
  <script defer src="/js/fontawesome.js"></script>
</head>

<body>

<header class="site-header">
  <nav class="site-navi">
    <h1 class="site-title"><a href="/">Kaiying</a></h1>
    <ul class="site-navi-items">
      <li class="site-navi-item-categories"><a href="/categories/" title="Categories">Categories</a></li>
      <li class="site-navi-item-tags"><a href="/tags/" title="Tags">Tags</a></li>
      <li class="site-navi-item-archives"><a href="/archives/" title="Archives">Archives</a></li>
      <li class="site-navi-item-about"><a href="/about/" title="About">About</a></li>
    </ul>
  </nav>
</header>
<hr class="site-header-bottom">

  <div class="breadcrumb">
    K8s
  </div>
  <div class="main" role="main">
    <section class="list section-list">
      <article class="article">
        <a href="/k8s/kubelet-structure/" class="article-titles">
          <h2 class="article-title">Kubelet 架构分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 23, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kubelet是k8s集群中节点的代理人，通过watch api server，来处理发送给本节点的任务，看一下官方对kubelet的描述：
The kubelet is the primary &quot;node agent&quot; that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms (primarily through the apiserver) and ensures that the containers described in those PodSpecs are running and healthy.
        </div>
        <div class="article-readmore"><a href="/k8s/kubelet-structure/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/kube-scheduler-source-code-read/" class="article-titles">
          <h2 class="article-title">K8S Scheduler 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 22, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 kube-scheduler 是master节点中重要的组件之一，它同watch接口监听api server中需要调度的pod，然后通过其预选，优选算法来确定pod需要被调度到哪个节点上去。
看一下官方对scheduler 调度流程的描述：The Kubernetes Scheduler
主要步骤如下：
 首先通过预选阶段过滤掉不合适的节点，例如根据podSpec中对系统资源的需求，会计算出所有节点剩余的资源是否可以满足pod需求，如果不满足则该节点会被抛弃。 其次通过预选阶段的节点，会再次进行优选。比如在满足pod资源需求的基础上，会优选出当前负载最小节点，保证每个集群节点资源被均衡使用。 最后选择打分最高的节点作为目标节点，如果得分相同则会随机选择一个，进行调度。  For given pod: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Pred. filters: node 3 doesn't have enough resource +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | remaining nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Priority function: node 1: p=2 node 2: p=5 +-------------------+-------------------------+ | | v select max{node priority} = node 2 入口 scheduler使用cobra创建命令行客户端，进入NewSchedulerCommand
        </div>
        <div class="article-readmore"><a href="/k8s/kube-scheduler-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/replicaset-source-code-read/" class="article-titles">
          <h2 class="article-title">Replica Set Controller 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 21, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境 kubernetes版本：kubernetes:v1.16.0
go环境：go version go1.13.4 darwin/amd64
前言 前面文章已经总结到，deployment controller通过replicaset来控制pod的创建和删除，进而实现更高级操作例如，滚动更新，回滚等。本文继续从源码角度 分析replicaset controller如何控制pod的创建和删除。
在分析源码前先考虑一下 replicaset 的使用场景，在平时的操作中其实我们并不会直接操作 replicaset，replicaset 也仅有几个简单的操作，创建、删除、更新等。
源码分析 入口 我们在上一篇文章中也说道controller manager是所有k8s 资源controller的管控中心，包含的主要逻辑就是高可用选择leader，然后 遍历所有的controllers,对每一个controller进行初始化并启动，也就是说只要集群启动，所有的controller也会都启动起来，开始list-watch api server，来准备创建资源。
cmd/kube-controller-manager/app/controllermanager.go:386 func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[&#34;endpoint&#34;] = startEndpointController controllers[&#34;endpointslice&#34;] = startEndpointSliceController ... controllers[&#34;deployment&#34;] = startDeploymentController controllers[&#34;replicaset&#34;] = startReplicaSetController } 接下来进入startReplicaSetController, 发现通过NewReplicaSetController初始化了ReplicaSetController，并并调用run方法启动controller。
cmd/kube-controller-manager/app/apps.go:69 func startReplicaSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: &#34;apps&#34;, Version: &#34;v1&#34;, Resource: &#34;replicasets&#34;}] { return nil, false, nil } go replicaset.NewReplicaSetController( ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(&#34;replicaset-controller&#34;), replicaset.BurstReplicas, ).Run(int(ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs), ctx.Stop) return nil, true, nil } 我们先分析NewReplicaSetController看一下初始化ReplicaSetController的具体步骤：发现他会监听rs的add,update,delete操作，和pod的add,update,delete操作。 当这两种资源发送变化时，将key(name+namespace)添加到queue队列中去。
//pkg/controller/replicaset/replica_set.go:126  func NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.
        </div>
        <div class="article-readmore"><a href="/k8s/replicaset-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/controller-manager-source-code-read/" class="article-titles">
          <h2 class="article-title">K8S Controller Manager 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 19, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  前言 Controller Manager内部包含Replication Controller、Node Controller、 ResourceQuota Controller、Namespace Controller、ServiceAccount Controller、Token Controller、Service Controller及Endpoint Controller这8种Controller， 每个Controller，它们通过API Server提供的（List-Watch）接口实时监控集群中特定资源的状态变化，当发生各种故障 导致某资源对象的状态发生变化时，Controller会尝试将其状态调整为期望的状态。
代码总体解析 入口 # cmd/kube-controller-manager/controller-manager.go func main() { rand.Seed(time.Now().UnixNano()) command := app.NewControllerManagerCommand() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { os.Exit(1) } } 进入NewControllerManagerCommand 函数，可以看到新建ControllerManager需要两步 启动参数配置，及Run函数，进入Run函数。
func NewControllerManagerCommand() *cobra.Command { s, err := options.NewKubeControllerManagerOptions() if err != nil { klog.Fatalf(&#34;unable to initialize command options: %v&#34;, err) } cmd := &amp;cobra.Command{ Use: &#34;kube-controller-manager&#34;, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, &#34;%v\n&#34;, err) os.
        </div>
        <div class="article-readmore"><a href="/k8s/controller-manager-source-code-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/api-server-source-read/" class="article-titles">
          <h2 class="article-title">K8S Api Server 源码分析</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>April 13, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          版本环境   kubernetes版本：kubernetes:v1.16.0
  go环境：go version go1.13.4 darwin/amd64
  Api Server架构 kubernetes api server 提供了增删改查，list,watch各类资源的接口，对请求进行验证，鉴权及admission controller, 请求成功后将结果更新到后端存储etcd（也可以是其他存储）中。
api server 架构从上到下可以分为以下几层：
1.Api层，主要包含三种api core api: 主要在/api/v1下
分组api: 其path为/apis/$NAME/$VERSION
暴露系统状态的api: 如/metrics,/healthz,/logs,/openapi等
$ curl -k https://127.0.0.1:56521 { &#34;paths&#34;: [ &#34;/api&#34;, &#34;/api/v1&#34;, &#34;/apis&#34;, &#34;/apis/&#34;, &#34;/apis/admissionregistration.k8s.io&#34;, &#34;/apis/admissionregistration.k8s.io/v1beta1&#34;, ... &#34;/healthz&#34;, &#34;/healthz/autoregister-completion&#34;, &#34;/healthz/etcd&#34;, ... &#34;/logs&#34;, &#34;/metrics&#34;, &#34;/openapi/v2&#34;, &#34;/swagger-2.0.0.json&#34;, &#34;/swaggerapi&#34;, &#34;/version&#34; 2.访问控制层 当用户访问api接口时，访问控制层将会做如下校验：
对用户进行身份校验(authentication)&ndash;&gt; 校验用户对k8s资源对象访问权限(authorization) &ndash;&gt; 根据配置的资源访问许可逻辑(admission control)，判断是否访问。
下面分别对这三种校验方式进行说明：
2.1 Authentication 身份校验 k8s提供了三种客户端身份校验方式：https双向证书认证，http token方式认证， http base 认证也就是用户名密码方式
a. 基于CA根证书签名的双向数字证书认证
client和server向CA机构申请证书
client-&gt;server，server下发服务端证书，client利用证书验证server是否合法
client发送客户端证书给server，server利用证书校验client是否合法
client和server利用随机密钥加密消息，然后通信
apiserver启动的时候通过&ndash;client-ca-file=XXXX配置签发client证书的CA，当client发送证书过来时，apiserver使用CA验证，如果证书合法，则可进行通信。
这种方式的优点是安全，缺点是无法撤销用户证书，创建集群就绑定了证书。
$ k get pods kube-apiserver-kind-control-plane -n kube-system -o yaml - command: - kube-apiserver - --authorization-mode=Node,RBAC - --advertise-address=172.17.0.2 - --allow-privileged=true - --client-ca-file=/etc/kubernetes/pki/ca.crt b.
        </div>
        <div class="article-readmore"><a href="/k8s/api-server-source-read/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/dns-problem/" class="article-titles">
          <h2 class="article-title">CoreDNS生产案：pod出现dns解析大量失败的问题</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>March 31, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          问题 收到阿里云K8S集群监控告警：
CoreDNS 5分钟内NXDOMAIN响应百分比大于50% k8s.coredns.response[aliyun,172.22.82.25:9153,NXDOMAIN]
是coredns的172.22.82.25的这个pod出现dns解析大量失败的问题。（解析成功的日志是NOERROR，解析失败是：NXDMAIN）
分析 首先是查看这个coredns pod的日志：
[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl get pods -n kube-system -o wide | grep coredns |grep 172.22.82.25 coredns-57dc86754b-9schh 1/1 Running 0 17d 172.22.82.25 cn-hangzhou.xxx.xxx.xxx.xxx &lt;none&gt;[root@iZbp16er8wkobo2a165ekzZ ~]# kubectl logs -n kube-system coredns-57dc86754b-9schh | tail -10 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:54106 - 24311 &quot;A IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&quot; NOERROR qr,rd,ra 190 0.00003395s 2020-03-05T13:06:29.745Z [INFO] 172.22.0.66:56730 - 20278 &quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com. udp 50 false 512&quot; NOERROR qr,rd,ra 232 0.000072002s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:41047 - 53211 &quot;AAAA IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&quot; NXDOMAIN qr,rd,ra 173 0.00004196s 2020-03-05T13:06:29.748Z [INFO] 172.22.0.66:40581 - 40714 &quot;A IN metrics.cn-hangzhou.aliyuncs.com.kube-system.svc.cluster.local. udp 80 false 512&quot; NXDOMAIN qr,rd,ra 173 0.
        </div>
        <div class="article-readmore"><a href="/k8s/dns-problem/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
      <article class="article">
        <a href="/k8s/my-first-post/" class="article-titles">
          <h2 class="article-title">Kubernetes中leaderelection实现组件高可用</h2>
          
        </a>
        <ul class="article-meta">
          <li class="article-meta-date"><time>February 11, 2020</time></li>
          <li class="article-meta-categories">
            <a href="/categories/k8s/">
              <i class="fas fa-folder"></i>
              k8s
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/k8s/">
              <i class="fas fa-tag"></i>
              k8s
            </a>&nbsp;
          </li>
          <li class="article-meta-tags">
            <a href="/tags/elect/">
              <i class="fas fa-tag"></i>
              elect
            </a>&nbsp;
          </li>
        </ul>
        <div class="article-content">
          在Kubernetes中，通常kube-schduler和kube-controller-manager都是多副本进行部署的来保证高可用，而真正在工作的实例其实只有一个。
这里就利用到 leaderelection 的选主机制，保证leader是处于工作状态，并且在leader挂掉之后，从其他节点选取新的leader保证组件正常工作。今天就来看看这个包的使用以及它内部是如何实现的。
基本使用 以下是一个简单使用的例子，编译完成之后同时启动多个进程，但是只有一个进程在工作，当把leader进程kill掉之后，会重新选举出一个leader进行工作，即执行其中的 run 方法：
/* 例子来源于client-go中的example包中， */ package main import ( &#34;context&#34; &#34;flag&#34; &#34;os&#34; &#34;os/signal&#34; &#34;syscall&#34; &#34;time&#34; &#34;github.com/google/uuid&#34; metav1 &#34;k8s.io/apimachinery/pkg/apis/meta/v1&#34; clientset &#34;k8s.io/client-go/kubernetes&#34; &#34;k8s.io/client-go/rest&#34; &#34;k8s.io/client-go/tools/clientcmd&#34; &#34;k8s.io/client-go/tools/leaderelection&#34; &#34;k8s.io/client-go/tools/leaderelection/resourcelock&#34; &#34;k8s.io/klog&#34; ) func buildConfig(kubeconfig string) (*rest.Config, error) { if kubeconfig != &#34;&#34; { cfg, err := clientcmd.BuildConfigFromFlags(&#34;&#34;, kubeconfig) if err != nil { returnnil, err } return cfg, nil } cfg, err := rest.InClusterConfig() if err != nil { returnnil, err } return cfg, nil } func main() { klog.InitFlags(nil) var kubeconfig string var leaseLockName string var leaseLockNamespace string var id string flag.StringVar(&amp;kubeconfig, &#34;kubeconfig&#34;, &#34;&#34;, &#34;absolute path to the kubeconfig file&#34;) flag.
        </div>
        <div class="article-readmore"><a href="/k8s/my-first-post/">Read more...</a></div>
        <div class="article-floatclear"></div>
      </article>
    </section>
    


  </div>



<div class="site-footer">
  <div class="copyright">&copy; Copyright 2017 Your name</div>
  <ul class="site-footer-items">
    <li class="site-footer-item-rsslink">
      <a href="/k8s/index.xml" type="application/rss+xml" target="_blank" title="RSS">
        <i class="fas fa-rss"></i>
      </a>
    </li>
    <li class="site-footer-item-about"><a href="/about/" title="About">About</a></li>
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://github.com/taikii/whiteplain">Whiteplain</a>
  </div>
</div>


</body>
</html>
