<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>Ceph学习笔记 - Kaiying</title>
  <meta property="og:title" content="Ceph学习笔记 - Kaiying" />
  <meta name="twitter:title" content="Ceph学习笔记 - Kaiying" />
  <meta name="description" content="$\color{#4285f4}{介绍}\color ceph 独一无二地用统一的系统提供了三大存储方式：
ceph 块存储
ceph 文件存储(cephFS)
ceph 对象存储
快速部署 ceph部署方式有很多中，常用方式有使用ceph-deploy来进行部署，这里为了测试使用docker方式快速搭建出一套ceph单机环境出来
docker 部署 $ rm -fr /etc/ceph $ rm -fr /var/lib/ceph $ mkdir -p /etc/ceph $ mkdir -p /var/lib/ceph $ docker run -d --net=host --privileged=true -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -e MON_IP=192.168.0.2 -e CEPH_PUBLIC_NETWORK=192.168.0.1/23 -e CEPH_DEMO_UID=qqq -e CEPH_DEMO_ACCESS_KEY=qqq -e CEPH_DEMO_SECRET_KEY=qqq -e CEPH_DEMO_BUCKET=qqq --name cephdemo ceph/daemon demo $ curl http://192.168.0.0:5000 $ docker exec -it cephdemo ceph -s 架构及组件   RADOS (Reliable Automomic Distributed Object Store)是ceph集群的基础，ceph中一切都是对象的形式存储，RADOS负责存储这些对象。确保数据一致性和可靠性。 ceph数据访问方法(如RBD,CephFS,RADOSGW,librados)所有操作方法都是在RADOS之上构建的。
  OSD ceph对象存储设备，负责存储用户实际数据，一个OSD守护进程和一个物理磁盘绑定，物理磁盘的数量和OSD守护进程的数量是一致的。 ceph osd由一个已经存在的linux文件系统物理磁盘驱动器和OSD服务组成。
 ceph OSD 文件系统 --&gt;btrfs或XFS或ext4 物理磁盘    通过命令查看单个节点上OSD的状态：
$ service ceph status osd #查看所有osd的ID $ service osd ls #检查osd map的状态 $ ceph osd stat #检查osd 树形图 $ ceph osd tree  MON Ceph monitor 跟踪整个集群的健康状态 一个典型的ceph集群通常包含多个monitor节点，且节点数为奇数个。使用Paxos算法来实现数据一致性。  $ service ceph status mon $ ceph mon stat $ ceph mon_status $ ceph mon dump   librados 提供了访问RADOS的接口，支持PHP,Ruby,Java,Python等语言">
  <meta property="og:description" content="$\color{#4285f4}{介绍}\color ceph 独一无二地用统一的系统提供了三大存储方式：
ceph 块存储
ceph 文件存储(cephFS)
ceph 对象存储
快速部署 ceph部署方式有很多中，常用方式有使用ceph-deploy来进行部署，这里为了测试使用docker方式快速搭建出一套ceph单机环境出来
docker 部署 $ rm -fr /etc/ceph $ rm -fr /var/lib/ceph $ mkdir -p /etc/ceph $ mkdir -p /var/lib/ceph $ docker run -d --net=host --privileged=true -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -e MON_IP=192.168.0.2 -e CEPH_PUBLIC_NETWORK=192.168.0.1/23 -e CEPH_DEMO_UID=qqq -e CEPH_DEMO_ACCESS_KEY=qqq -e CEPH_DEMO_SECRET_KEY=qqq -e CEPH_DEMO_BUCKET=qqq --name cephdemo ceph/daemon demo $ curl http://192.168.0.0:5000 $ docker exec -it cephdemo ceph -s 架构及组件   RADOS (Reliable Automomic Distributed Object Store)是ceph集群的基础，ceph中一切都是对象的形式存储，RADOS负责存储这些对象。确保数据一致性和可靠性。 ceph数据访问方法(如RBD,CephFS,RADOSGW,librados)所有操作方法都是在RADOS之上构建的。
  OSD ceph对象存储设备，负责存储用户实际数据，一个OSD守护进程和一个物理磁盘绑定，物理磁盘的数量和OSD守护进程的数量是一致的。 ceph osd由一个已经存在的linux文件系统物理磁盘驱动器和OSD服务组成。
 ceph OSD 文件系统 --&gt;btrfs或XFS或ext4 物理磁盘    通过命令查看单个节点上OSD的状态：
$ service ceph status osd #查看所有osd的ID $ service osd ls #检查osd map的状态 $ ceph osd stat #检查osd 树形图 $ ceph osd tree  MON Ceph monitor 跟踪整个集群的健康状态 一个典型的ceph集群通常包含多个monitor节点，且节点数为奇数个。使用Paxos算法来实现数据一致性。  $ service ceph status mon $ ceph mon stat $ ceph mon_status $ ceph mon dump   librados 提供了访问RADOS的接口，支持PHP,Ruby,Java,Python等语言">
  <meta name="twitter:description" content="$\color{#4285f4}{介绍}\color ceph 独一无二地用统一的系统提供了三大存储方式：
ceph 块存储
ceph 文件存储(cephFS)
ceph 对象存储
快速部署 ceph部署方式有很多中，常用方式有使用ceph-deploy来进行部署，这里为了测试使用docker方式快速搭建出一套ceph单机环境出来
docker 部署 $ rm -fr /etc/ceph $ rm …">
  <meta name="author" content="kaiying"/>
  <meta property="og:site_name" content="Kaiying" />
  <meta property="og:url" content="https://wukaiying.github.io/k8sstorage/ceph/" />
  <meta property="og:type" content="article" />
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.61.0" />

  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="/js/script.js"></script>
  <script src="/js/custom.js"></script>
  <script defer src="/js/fontawesome.js"></script>
</head>

<body>

<header class="site-header">
  <nav class="site-navi">
    <h1 class="site-title"><a href="/">Kaiying</a></h1>
    <ul class="site-navi-items">
      <li class="site-navi-item-categories"><a href="/categories/" title="Categories">Categories</a></li>
      <li class="site-navi-item-tags"><a href="/tags/" title="Tags">Tags</a></li>
      <li class="site-navi-item-archives"><a href="/archives/" title="Archives">Archives</a></li>
      <li class="site-navi-item-about"><a href="/about/" title="About">About</a></li>
    </ul>
  </nav>
</header>
<hr class="site-header-bottom">

  <div class="main" role="main">
    <article class="article">
      
      
      <h1 class="article-title">Ceph学习笔记</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-date"><time>May 19, 2020</time></li>
      </ul>
      
<aside class="toc">
  <nav id="TableOfContents">
  <ul>
    <li><a href="#ea4335">$\color{#4285f4}{介绍}\color</a></li>
    <li><a href="#heading">快速部署</a>
      <ul>
        <li></li>
        <li><a href="#heading-1">架构及组件</a></li>
        <li><a href="#ceph-">ceph 内部模块</a></li>
        <li><a href="#ceph--1">ceph 配置</a></li>
      </ul>
    </li>
  </ul>
</nav>
</aside>
      <h2 id="ea4335">$\color{#4285f4}{介绍}\color</h2>
<p>ceph 独一无二地用统一的系统提供了三大存储方式：</p>
<p>ceph 块存储</p>
<p>ceph 文件存储(cephFS)</p>
<p>ceph 对象存储</p>
<h2 id="heading">快速部署</h2>
<p>ceph部署方式有很多中，常用方式有使用ceph-deploy来进行部署，这里为了测试使用docker方式快速搭建出一套ceph单机环境出来</p>
<h4 id="docker-">docker 部署</h4>
<pre><code>$ rm -fr /etc/ceph
$ rm -fr /var/lib/ceph
$ mkdir -p /etc/ceph
$ mkdir -p /var/lib/ceph

$ docker run -d --net=host --privileged=true -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -e MON_IP=192.168.0.2 -e CEPH_PUBLIC_NETWORK=192.168.0.1/23 -e CEPH_DEMO_UID=qqq  -e CEPH_DEMO_ACCESS_KEY=qqq  -e CEPH_DEMO_SECRET_KEY=qqq  -e CEPH_DEMO_BUCKET=qqq --name cephdemo ceph/daemon demo
$ curl http://192.168.0.0:5000
$ docker exec -it cephdemo ceph -s
</code></pre><h3 id="heading-1">架构及组件</h3>
<p><img src="https://ae01.alicdn.com/kf/Hfde8af3989fb425c93d97f12a971695ct.jpg" alt=""></p>
<ul>
<li>
<p>RADOS (Reliable Automomic Distributed Object Store)是ceph集群的基础，ceph中一切都是对象的形式存储，RADOS负责存储这些对象。确保数据一致性和可靠性。
ceph数据访问方法(如RBD,CephFS,RADOSGW,librados)所有操作方法都是在RADOS之上构建的。</p>
</li>
<li>
<p>OSD ceph对象存储设备，负责存储用户实际数据，一个OSD守护进程和一个物理磁盘绑定，物理磁盘的数量和OSD守护进程的数量是一致的。
ceph osd由一个已经存在的linux文件系统物理磁盘驱动器和OSD服务组成。</p>
<pre><code>                      ceph OSD
                      文件系统        --&gt;btrfs或XFS或ext4
                      物理磁盘
</code></pre>
</li>
</ul>
<p>通过命令查看单个节点上OSD的状态：</p>
<pre><code>$ service ceph status osd
#查看所有osd的ID
$ service osd ls
#检查osd map的状态
$ ceph osd stat
#检查osd 树形图
$ ceph osd tree
</code></pre><ul>
<li>MON Ceph monitor 跟踪整个集群的健康状态
一个典型的ceph集群通常包含多个monitor节点，且节点数为奇数个。使用Paxos算法来实现数据一致性。</li>
</ul>
<pre><code>$ service ceph status mon
$ ceph mon stat
$ ceph mon_status
$ ceph mon dump
</code></pre><ul>
<li>
<p>librados 提供了访问RADOS的接口，支持PHP,Ruby,Java,Python等语言</p>
</li>
<li>
<p>RBD ceph块设备，对外提供块存储
块存储是企业环境中最常见的一种数据存储方式，ceph块设备也称之为RADOS块设备(RBD)</p>
</li>
<li>
<p>RGW ceph对象网关，提供了兼容s3和openstack对象存储swift的接口
也称之为RADOS网关，它是一个代理可以将http请求转换为RADOS，同时也可以将RADOS请求转化为http请求。
<img src="https://ae01.alicdn.com/kf/H0fad099edbd745e39b7f7ead00a88764Q.jpg" alt=""></p>
</li>
<li>
<p>MDS ceph元数据服务器
只有ceph文件系统(CephFS)才需要，其他存储方法不需要。</p>
</li>
</ul>
<p>cephFS ceph文件系统</p>
<h3 id="ceph-">ceph 内部模块</h3>
<ul>
<li>对象</li>
</ul>
<p>一个对象包括数据及元数据，并用一个唯一id作为标识。ceph以对象的方式进行存储。</p>
<ul>
<li>
<p>CRUSH（controlled Replication Under Scalable Hashing）
与传统系统依赖于存储和管理一个核心元数据/索引表不同的是，ceph使用crush算法来准确的计算数据应该被写到哪里或者去哪里读取。</p>
<p>crush 查找</p>
<p>对于ceph集群的以此读写操作，客户端首先联系cpeh的monitor获取一个集群的map副本。通过map获取ceph获取集群状态和配置信息。然后将数据转化为对象
，并将对象和PG(placement groups)经过散列来生成它在ceph池中最终存放在哪个一个PG。拿到前面计算好的PG经过CRUSH查找来确定存储或获取数据的主OSD的位置。
拿到目标OSD的ID，客户端直接联系该OSD存储数据，等数据写完后，主OSD节点会执行复制操作，实现数据高可用。</p>
</li>
<li>
<p>PG(Placement Group)</p>
</li>
</ul>
<p>所有数据被抽象成多个object，每个object都会对应到唯一的一个pg上（多副本表示有多个相同的pg，当然object也自然是多副本的），然后pg映射到osd上存储。为什么不将对象直接映射在OSD上是因为
object数量就太多了，这样可能会加大复杂性，而且也会带来不小的开销，于是引进pg这个概念，把object装进pg中，以pg为存储单元个体，直接追踪pg状态，一般pg数量是远远小于object数量的。</p>
<ul>
<li>ceph池</li>
</ul>
<p>ceph每个池中都包含一定数量的PG,可以实现将一定数量的对象映射到不同的OSD上。</p>
<ul>
<li>ceph数据管理
数据管理始于客户端向ceph池中写数据，一旦客户端准备写数据到ceph翅中，数据首先写到主osd中，主osd在复制相同的数据到第二osd和第三osd中，确认写入完成。当所有的osdd都完成后，主osd返回应答信号给客户端，确认写完成。</li>
</ul>
<h3 id="ceph--1">ceph 配置</h3>
<pre><code>$ rbd create ceph-client1-rbd1 --size 10240
$ rbd ls
$ rbd --image ceph-client1-rbd1 info
#为image指定rbd存储池
$ rbd --image ceph-client1-rbd1 info -p rbd
</code></pre><ul>
<li>创建资源池pool</li>
</ul>
<p>我们先以Ceph RBD的方式来介绍Ceph集群的使用，通过在Ceph集群中创建一个RBD块文件供用户进行使用，要使用Ceph，首先需要一个资源池pool，pool是Ceph中数据存储抽象的概念，其由多个pg（Placegroup)和pgp组成，创建的时候可以指定pg的数量，pg的大小一般为2^n次方，如下先创建一个pool</p>
<p>1、创建一个pool，其名字为happylau，包含64个PG/PGP，</p>
<pre><code>[root@node-1 ~]# ceph osd pool create happylau 64 64 
pool 'happylau' created
</code></pre><p>2、可以查看pool的信息，如查看当前集群的pool列表——lspools,可以使用df查看资源池的存储空间</p>
<p>查看pool列表
[root@node-1 ~]# ceph osd lspools
1 happylau</p>
<p>查看存储空间使用情况</p>
<pre><code>[root@node-1 ~]# ceph df
GLOBAL:
    SIZE        AVAIL       RAW USED     %RAW USED 
    147 GiB     144 GiB      3.0 GiB          2.07 
POOLS:
    NAME         ID     USED       %USED     MAX AVAIL     OBJECTS 
    happylau     1      14 MiB      0.03        46 GiB          24 
</code></pre><p>3、查看pg_num和pgp_num,副本数size大小</p>
<p>查看pg和pgp数量</p>
<pre><code>[root@node-1 ~]# ceph osd pool get happylau pg_num
pg_num: 64
[root@node-1 ~]# ceph osd pool get happylau pgp_num
pgp_num: 64
</code></pre><p>查看size大小,默认为三副本</p>
<pre><code class="language-[root@node-1" data-lang="[root@node-1">   size: 3
</code></pre><p>关于pg_num和pgp_num的数量，官方推荐的计算方案为，每个OSD推荐使用100个PG，OSD的数量<em>100/pool副本数量，然后取最近2的^n次方的值，套公式，其值为：PGs=3</em>100/3=100，取最近的值为128，此时的设置低于官方建议。</p>
<pre><code>             (OSDs * 100)
Total PGs =  ------------
              pool size
</code></pre><p>4、调整pool的pg_num和pgp_num数量,调整pg和pgp会涉及到数据迁移，如果线上业务情评估好，在低峰期做操作，避免数据rebalancing造成业务影响</p>
<pre><code>[root@node-1 ~]# ceph osd pool set happylau pg_num 128
[root@node-1 ~]# ceph osd pool set happylau pgp_num 128
</code></pre><p>5、调整副本数，可以根据需要调整资源池pool的副本数，默认为三个，也可以手动调整pool副本数量的个数，以下调整pool资源池副本数量为2个副本</p>
<pre><code>[root@node-1 ~]# ceph osd pool set happylau size 2
set pool 1 size to 2
</code></pre><p>查看校验pool副本个数</p>
<pre><code>[root@node-1 ~]# ceph osd pool get happylau size
size: 2
</code></pre><p>此时，如果集群中有数据，Ceph会自动做数据均衡（rebalancing），将多余的pg同步到其他osd，确保当前pool资源池的副本数为2个，此时使用ceph -s查看集群，可以看到pool资源池的pg的数量</p>
<pre><code>[root@node-1 ~]# ceph -s 
  cluster:
    id:     760da58c-0041-4525-a8ac-1118106312de
    health: HEALTH_WARN
            application not enabled on 1 pool(s)
 
  services:
    mon: 1 daemons, quorum node-1
    mgr: node-1(active)
    osd: 3 osds: 3 up, 3 in
 
  data:
    pools:   1 pools, 128 pgs  #PGs数量
    objects: 24  objects, 14 MiB
    usage:   3.0 GiB used, 144 GiB / 147 GiB avail
    pgs:     128 active+clean
 
  io:
    client:   3.2 KiB/s wr, 0 op/s rd, 0 op/s wr
</code></pre><p>由上可以看到Ceph集群当前的整体情况，包含有cluster，services，data，io信息，health状态可以看到集群当前处于告警HEALTH_WARN状态，告警提示内容为“application not enabled on 1pool(s)”，资源池pool未启用application，Ceph推荐开启application的功能。</p>
<p>6、开启application功能,application即设置资源池pool存储接口类型，包含三种：rbd，cephfs和,rgw，可以根据资源池pool的类型，设置不同的application，如下开启资源池为rbd类型</p>
<pre><code>[root@node-1 ~]# ceph osd pool application enable happylau rbd
enabled application 'rbd' on pool 'happylau'

[root@node-1 ~]# ceph osd pool application get happylau
{
    &quot;rbd&quot;: {}
}
再次查看Ceph，可以看到Ceph此时处于Health状态

[root@node-1 ~]# ceph -s 
  cluster:
    id:     760da58c-0041-4525-a8ac-1118106312de
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum node-1
    mgr: node-1(active)
    osd: 3 osds: 3 up, 3 in
 
  data:
    pools:   1 pools, 128 pgs
    objects: 24  objects, 14 MiB
    usage:   3.0 GiB used, 144 GiB / 147 GiB avail
    pgs:     128 active+clean
</code></pre><p>至此，Ceph资源池pool相关操作到此完毕，接下来在pool中创建镜像。</p>
<ul>
<li>RBD块存储使用
Ceph RBD块存储主要为虚拟化如KVM提供块存储设备，KVM需要依赖于qemu和libvirt和RBD块存储交互，这也是和云平台如OpenStack，CloudStack，Kubernetes交互的基础，因此非常有必要掌握Ceph原生RBD块存储的使用。由于当前Ceph集群未与虚拟化或者云平台进行集成，因此采用原生的rbd接口与Ceph进行交互（云平台通过调用rbd接口完成块存储生命周期管理）。</li>
</ul>
<p>1、rbd块存储接口交互的工具为rbd，通过rbd命令来实现RBD块的创建，如创建一个10G的块存储</p>
<pre><code>[root@node-1 ~]# rbd create -p happylau --image ceph-rbd-demo.img --size 10G 

</code></pre><p>2、如上创建了一个ceph-rbd-demo.img的RBD块文件，大小为10G，可以通过ls和info查看RBD镜像的列表和详情信息</p>
<p>查看RBD镜像列表</p>
<pre><code>[root@node-1 ~]# rbd -p happylau ls 
ceph-rbd-demo.img
</code></pre><p>查看RBD详情，可以看到镜像包含2560个objects，每个ojbect大小为4M，对象以rbd_data.10b96b8b4567开头</p>
<pre><code>[root@node-1 ~]# rbd -p happylau info ceph-rbd-demo.img
rbd image 'ceph-rbd-demo.img':
	size 10 GiB in 2560 objects
	order 22 (4 MiB objects)
	id: 10b96b8b4567
	block_name_prefix: rbd_data.10b96b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features: 
	flags: 
	create_timestamp: Mon Mar  2 15:32:39 2020
</code></pre><p>3、RBD存储块已创建起来了，如何使用呢？如果已和虚拟化环境结合，创建好虚拟机然后在磁盘中写数据即可，但此时还未与虚拟化结合（结合难度也比较大，后续再专门讨论），rbd提供了一个map的工具，可以将一个RBD块映射到本地块进行使用，大大简化了使用过程，rbd map时候，exclusive-lock, object-map, fast-diff, deep-flatten的features不支持，因此需要先disable，否则会提示RBD image feature set mismatch报错信息</p>
<p>关闭默认的featrues</p>
<pre><code>[root@node-1 ~]# rbd -p happylau --image ceph-rbd-demo.img feature disable deep-flatten
[root@node-1 ~]# rbd -p happylau --image ceph-rbd-demo.img feature disable fast-diff
[root@node-1 ~]# rbd -p happylau --image ceph-rbd-demo.img feature disable object-map
[root@node-1 ~]# rbd -p happylau --image ceph-rbd-demo.img feature disable exclusive-lock
</code></pre><p>查看校验featrue信息</p>
<pre><code>[root@node-1 ~]# rbd -p happylau info ceph-rbd-demo.img
rbd image 'ceph-rbd-demo.img':
	size 10 GiB in 2560 objects
	order 22 (4 MiB objects)
	id: 10b96b8b4567
	block_name_prefix: rbd_data.10b96b8b4567
	format: 2
	features: layering
	op_features: 
	flags: 
	create_timestamp: Mon Mar  2 15:32:39 2020
</code></pre><p>将RBD块map到本地,此时map后，可以看到RBD块设备映射到了本地的一个/dev/rbd0设备上</p>
<pre><code>[root@node-1 ~]# rbd map -p happylau --image ceph-rbd-demo.img
/dev/rbd0
[root@node-1 ~]# ls -l /dev/rbd0
brw-rw---- 1 root disk 251, 0 3月   2 15:58 /dev/rbd0
</code></pre><p>4、RBD块设备已映射到本地的/dev/rbd0设备上，因此可以对设备进行格式化操作使用</p>
<p>通过device list可以查看到当前机器RBD块设备的映射情况</p>
<pre><code>[root@node-1 ~]# ls -l /dev/rbd0
brw-rw---- 1 root disk 251, 0 3月   2 15:58 /dev/rbd0
</code></pre><p>该设备可以像本地的一个盘来使用，因此可以对其进行格式化操作</p>
<pre><code>[root@node-1 ~]# mkfs.xfs /dev/rbd0 
meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@node-1 ~]# blkid /dev/rbd0
/dev/rbd0: UUID=&quot;35f63145-0b62-416d-81f2-730c067652a8&quot; TYPE=&quot;xfs&quot; 
</code></pre><p>挂载磁盘到系统中</p>
<pre><code>[root@node-1 ~]# mkdir /mnt/ceph-rbd
[root@node-1 ~]# mount /dev/rbd0 /mnt/ceph-rbd/
[root@node-1 ~]# df -h /mnt/ceph-rbd/
文件系统        容量  已用  可用 已用% 挂载点
/dev/rbd0        10G   33M   10G    1% /mnt/ceph-rbd
[root@node-1 ~]# cd /mnt/ceph-rbd/
[root@node-1 ceph-rbd]# echo &quot;testfile for ceph rbd&quot; &gt;rbd.log
</code></pre><p>5、通过rbd info可以看到镜像块存储对象的前缀为rbd_data.10b96b8b4567，即存储被切割为多个object，object的前缀以rbd_data.10b96b8b4567开头，可以通过rados查看存储池中的obejct,这些object会随着真实使用空间的增长而自动增长</p>
<p>6、至此，已经完成了rbd存储的创建和通过kernel的方式挂在到系统中使用，这种使用方式相对较少，一般以KVM的libvirt/qemu方式进行对接，如果需要卸载可以按照如下方式进行</p>
<pre><code>卸载文件系统
[root@node-1 ~]# umount  /dev/rbd0 

卸载块存储,再次使用rbd device ls查看显示为空则为卸载成功
[root@node-1 ~]# rbd device unmap -p happylau --image ceph-rbd-demo.img 
</code></pre>
    </article>

    


    <ul class="pager article-pager">
      <li class="pager-newer pager-noitem">&lt; Newer</li>
      <li class="pager-older pager-noitem">Older &gt;</li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">&copy; Copyright 2017 Your name</div>
  <ul class="site-footer-items">
    <li class="site-footer-item-about"><a href="/about/" title="About">About</a></li>
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://github.com/taikii/whiteplain">Whiteplain</a>
  </div>
</div>


</body>
</html>
